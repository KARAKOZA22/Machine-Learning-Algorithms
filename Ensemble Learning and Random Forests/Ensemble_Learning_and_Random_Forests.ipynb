{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning and Random Forests<br>\n",
        "**Enseble method:** you can train a group of decision tree\n",
        "classifiers, each on a different random subset of the training set. You can then obtain\n",
        "the predictions of all the individual trees, and the class that gets the most votes is the\n",
        "ensembleâ€™s prediction. <br>Such an ensemble of decision trees is called a random forest.<br>\n",
        "We will examine the most popular ensemble methods, including voting classifiers, bagging and pasting ensembles, random forests, and boosting, and stacking ensembles."
      ],
      "metadata": {
        "id": "c82NzocYvoj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Voting Classifiers"
      ],
      "metadata": {
        "id": "I-exN7huxWPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we train data to 4 different models let's say KNN, svm, decision tree, and logistic regression.<br><br>\n",
        "The most voted class will be taken as the prediction like if 3 models of 4 predict that the predicted class is robot, then the prediction will be according to the majority which is the robot, and this is called **hard voting classifier**.<br><br>\n",
        "Voting Classifiers get high accuracy than best classifier in ensemble even if each classifier in ensemble is weak, this is according to the **law of large numbers**.<br><br>\n",
        "To get high accuracy you have to make the predictors are independent as possible, as if they are dependent they will likely make the same error so, the ensemble method accuracy will be reduced."
      ],
      "metadata": {
        "id": "A9Ojrd1xx6Lq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PHI7AbZUvnQ6"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingRegressor, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "v_clf = VotingClassifier(\n",
        "estimators=[('lr', LogisticRegression(random_state=42)),\n",
        "('rf', RandomForestClassifier(random_state=42)),\n",
        "('svc', SVC(random_state=42))])\n",
        "v_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KywsRA7K2EnM",
        "outputId": "105b7d70-62fc-48b6-f039-20b7dd0c0afc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
              "                             ('rf', RandomForestClassifier(random_state=42)),\n",
              "                             ('svc', SVC(random_state=42))])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model,clf in v_clf.named_estimators_.items():\n",
        "  print(f\"The accuracy of {model}: {clf.score(X_test,y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUD4YzYq2Ykj",
        "outputId": "97483b1f-5b40-47d6-d6cb-ef659556050d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of lr: 0.864\n",
            "The accuracy of rf: 0.896\n",
            "The accuracy of svc: 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the best classifiers are random forest, and support vector machines lets try ensemble method"
      ],
      "metadata": {
        "id": "jI3BakQq3KEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict in voting classifier makes the hard voting\n",
        "print(v_clf.predict(X_test[:1]))\n",
        "print([v_clf.predict(X_test[:1]) for clf in v_clf.estimators])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEi6pAV52-F9",
        "outputId": "468cebe8-5a71-433e-8c2c-b73a967cd1f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[array([1]), array([1]), array([1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here all of the classifiers predicted the same class for this instance "
      ],
      "metadata": {
        "id": "AGVrD9xN4Buc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v_clf.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAnmwf6d3zi8",
        "outputId": "c050982e-9533-4d6b-d19b-e1f96e6c8edb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The voting classifier get higher accuracy than best classifier by 1.6 %**"
      ],
      "metadata": {
        "id": "FeOclEIP4Txw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all classifiers have predict_proba() method then, you can predict according to the highest class probability averaged overall individual classifiers this is called **soft voting**"
      ],
      "metadata": {
        "id": "Y9XaSFw87EC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v_clf.voting = 'soft'\n",
        "\"\"\"in svc the predict proba() is not available,\n",
        "when its probability heperparameter = False \n",
        "which is false by default, so it has to be = True \"\"\"\n",
        "v_clf.named_estimators['svc'].probability = True\n",
        "v_clf.fit(X_train,y_train)\n",
        "v_clf.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwxjpoJ_5IQ0",
        "outputId": "03017124-dc53-4e25-f9b1-6f6f515e1386"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WoW! it works we get a higher accuracy"
      ],
      "metadata": {
        "id": "FH3Xn1wx8wxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(v_clf.named_estimators)\n",
        "print(v_clf.estimators)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON2rdkYI8M1k",
        "outputId": "9ccd09f6-34c4-4324-f759-49b6c73facb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lr': LogisticRegression(random_state=42), 'rf': RandomForestClassifier(random_state=42), 'svc': SVC(probability=True, random_state=42)}\n",
            "[('lr', LogisticRegression(random_state=42)), ('rf', RandomForestClassifier(random_state=42)), ('svc', SVC(probability=True, random_state=42))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Bagging and Pasting"
      ],
      "metadata": {
        "id": "8itkt1-ceIYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach is to use the same training algorithm for every\n",
        "predictor but train them on different random subsets of the training set.<br> When sampling\n",
        "is performed with replacement, this method is called **bagging**. When sampling is performed without replacement, it is called **pasting**.\n",
        "<br>In other words, every classifier in ensemble take diffrent sample of data.<br>**Pasting**has absoute independence, while **Bagging** has less independence.<br>\n",
        "Bagging: short for bootstrap aggregation."
      ],
      "metadata": {
        "id": "CwjHuXqbNchZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" n_jobs tells Scikit-Learn the number of CPU\n",
        "cores to use for training and predictions \n",
        "and (-1) means use all available cores\n",
        "\n",
        "n_estimators  = no. of base estimator in ensemble\n",
        "\"\"\"\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "max_samples=100, n_jobs=-1, random_state=42)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "1FR7eONNd5gW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9c9c90-1843-45f7-c3b2-fe58331686c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
              "                  n_estimators=500, n_jobs=-1, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out of Bag (OOB) Evaluation <br>\n",
        "it can\n",
        "be shown mathematically that only about 63% of the training instances are sampled on\n",
        "average for each predictor. The remaining 37% of the training instances that are not\n",
        "sampled are called out-of-bag (OOB) instances. Note that they are not the same 37%\n",
        "for all predictors."
      ],
      "metadata": {
        "id": "boU_QkY9SIiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bagging ensemble can be evaluated using OOB instances, without the need for a\n",
        "separate validation set: indeed, if there are enough estimators, then each instance in the\n",
        "training set will likely be an OOB instance of several estimators, so these estimators\n",
        "can be used to make a fair ensemble prediction for that instance. Once you have a\n",
        "prediction for each instance, you can compute the ensembleâ€™s prediction accuracy (or\n",
        "any other metric)."
      ],
      "metadata": {
        "id": "QZwr9yEDUM_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "max_samples=100, n_jobs=-1, random_state=42,oob_score=True)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkIfkxbfTCan",
        "outputId": "ed68d0ee-a5e3-4011-8c3f-f4d47765a9c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9253333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Q_Q2haTtpW",
        "outputId": "7500987f-48e4-4eec-ed01-2d33edc15248"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often the accuracy score is higher than the oob_score but at this case the oob_score is higher which is something good"
      ],
      "metadata": {
        "id": "m8LThKc8TrSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Random Forests"
      ],
      "metadata": {
        "id": "EuJ2TlW5Uir1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest** is an ensemble of decision trees, generally\n",
        "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
        "set to the size of the training set. Instead of building a BaggingClassifier and\n",
        "passing it a DecisionTreeClassifier, you can use the\n",
        "RandomForestClassifier class, which is more convenient and optimized for\n",
        "decision trees (similarly, there is a RandomForestRegressor class for\n",
        "regression tasks)."
      ],
      "metadata": {
        "id": "IpQYmPs-oOOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n",
        "n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred_rf)"
      ],
      "metadata": {
        "id": "7PeixLyMUvPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdaf08a-52e7-4fd3-df39-8c2d29ff413f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra tree Classifier <br>\n",
        "Rondom forest splits random subset of feature at each node, it is possible to make trees even more random by using **splitter='random'** \n",
        "id decision tree, but in random forest we can use **ExtraTreesClassifier with the bootstrap=False**, this technique trades more bias for a lower variance, and it is much faster than regular random forests, but we are not sure if it will perform better than random forest or not so, the only way to be sure is to compare them using cross validation\n"
      ],
      "metadata": {
        "id": "CLcrNgVYqBbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16,\n",
        "n_jobs=-1, random_state=42,bootstrap=False)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbrVqmi1oeCT",
        "outputId": "2b4152cb-fb7b-41e0-8ffc-7c9a508180e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can know the importance of features by looking @ the impurity of nodes, we can access **feature_importances_** variable to show the importance of each feature"
      ],
      "metadata": {
        "id": "KQPczCGEslj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris(as_frame=True)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "rnd_clf.fit(iris.data, iris.target)\n",
        "for score, name in zip(rnd_clf.feature_importances_,iris.data.columns):\n",
        "  print(f\"Importance: {round(score, 2)*100}%, Feature name: {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqrUMeEAp7Ej",
        "outputId": "fc2c1048-676b-4810-d962-22001116e9f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importance: 11.0%, Feature name: sepal length (cm)\n",
            "Importance: 2.0%, Feature name: sepal width (cm)\n",
            "Importance: 44.0%, Feature name: petal length (cm)\n",
            "Importance: 42.0%, Feature name: petal width (cm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the most important features in the iris dataset are: petal lenght, and petal width."
      ],
      "metadata": {
        "id": "uqSZHcxZtWf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting\n",
        "It refers to any ensemble method that combine several weak learners into a strong one.<br>\n",
        "The general idea of boosting is to train the predictors sequentially each trying to correct the predrcessor."
      ],
      "metadata": {
        "id": "O54KgV0uvWF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4- AdaBoost <br>\n",
        "One way for a new predictor to correct its predecessor is to pay a bit more attention to\n",
        "the training instances that the predecessor underfit. This results in new predictors\n",
        "focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
        "for example the misclassified instances get more weight, then the next predictor predict on the updated weights.<br><br>\n",
        "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of\n",
        "estimators or more strongly regularizing the base estimator."
      ],
      "metadata": {
        "id": "Z2t_ro5ywHls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
        "learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbCRxoJPtVU0",
        "outputId": "80a4eed6-4509-490f-ee8e-8916bac59bcd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5- Gradient Boosting<br>\n",
        "It is like Adaboost, but instead of updating weights of training instances, it tries ti fit the new predictor to the residual error made by previous one.<br>It scales all the trees by the same amount<br>\n",
        "Residual error is the difference between the predicted values and actual values\n"
      ],
      "metadata": {
        "id": "G2NXqTdg0LVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best = GradientBoostingClassifier(\n",
        "max_depth=2, learning_rate=0.05, n_estimators=500,\n",
        "n_iter_no_change=10, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)\n",
        "y_pred_gbrt = gbrt_best.predict(X_test)\n",
        "accuracy_score(y_test,y_pred_gbrt)"
      ],
      "metadata": {
        "id": "sT5PzbHhzrj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b1ec53-b281-402b-cb55-72ad6a6e9309"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n_iter_no_change:** means to stop adding trees if there is no progress, if it is too low this may lead too early stop which will cause underfitting, while if it is too high it will overfit<br><br>"
      ],
      "metadata": {
        "id": "oNRQj5f8PxDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best.n_estimators_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RENgBz_PTZZ",
        "outputId": "5dbee097-661a-460c-842b-1fad76db7f47"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so, the best number of estimators are 152 not 500<br>\n",
        "**LETS TRY IT AGAIN**"
      ],
      "metadata": {
        "id": "6vbQUW8sQJvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best = GradientBoostingClassifier(\n",
        "max_depth=2, learning_rate=0.05, n_estimators=152,\n",
        "n_iter_no_change=10, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)\n",
        "y_pred_gbrt = gbrt_best.predict(X_test)\n",
        "accuracy_score(y_test,y_pred_gbrt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OW1qtoQGY2",
        "outputId": "9f644cc8-1d71-4d45-adcd-895fa0508a8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, when we put the n_estimators=152, and 500, the accuracy is the same, it doesn't overfit because of the early stopping due to the **n_iter_no_change**, as there is no progress so, it stops adding more trees at 152"
      ],
      "metadata": {
        "id": "Kmvxwt2KQrC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6- Stacking <br>\n",
        "It is based on a simple idea: instead of using trivial functions to aggregate the predictions of all predictors in an ensemble, why\n",
        "donâ€™t we train a model to perform this aggregation? such as making a predictor for final prediction (blender or meta learner)<br> **Example:** We entered a new instance for 4 predictors the values predict by 4 predictors are **[100, 112.5, 106, 109.7]** instead of taking the average value of predictors as the predicted value in **Regression** or raking the most voted class in **Classification**, here we take the predicted values from the predictors and make them as inputs for predictor (blender or meta learner) that will give us the predicted value from the model **[108.25]**"
      ],
      "metadata": {
        "id": "IgclwHKESZC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following photo will explain the process of Stacking more"
      ],
      "metadata": {
        "id": "z3xx4JkkcIF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/96451039/217559225-5400c4d9-1e59-4c2c-82b0-d119b8bf35bb.png)\n"
      ],
      "metadata": {
        "id": "LVWazMlHbpsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_clf = StackingClassifier(\n",
        "estimators=[\n",
        "('lr', LogisticRegression(random_state=42)),\n",
        "('rf', RandomForestClassifier(random_state=42)),\n",
        "('svc', SVC(probability=True, random_state=42))\n",
        "],\n",
        "# final estimator is the meta \n",
        "final_estimator=RandomForestClassifier(random_state=43),cv=5)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uJfi3zbSb7u",
        "outputId": "c81b1759-3922-41aa-b6e1-acda2d36b740"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.928"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your journey hasn't ended yet you have a lot to learn in ensemble methods such as XGBoost, CatBoost, and LightGBM."
      ],
      "metadata": {
        "id": "d-8a4E-6c5kn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KPVjlqfdUbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}